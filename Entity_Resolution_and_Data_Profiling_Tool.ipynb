{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing Entity Resolution and Data Profiling Techniques: A Comprehensive Approach"
      ],
      "metadata": {
        "id": "twqw2Fv4rtOU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gc5cpnF9Gxr"
      },
      "source": [
        "##Profiling relational data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOTF5kZfAK3z"
      },
      "source": [
        "\n",
        "For this task, download and read the paper about profiling relational data, select a set of summary statistics about the data (minimum of 10 different values) and write Python code to compute these quantities for a dataset of your choice. Preferably, you can use one of the csv files from the road safety dataset. Explain the importance of each summary statistic that you selected in understanding the characteristics of the dataset.\n",
        "\n",
        "Link to the dataset selected: https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-historical-revisions-data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyBHOS-WPcHz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset = pd.read_csv(\"dft-road-casualty-statistics-historical-revisions-data.csv\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0yZstGoPK0F"
      },
      "source": [
        "####A. Cardinalities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdQ7XQrZP503"
      },
      "outputs": [],
      "source": [
        "# 1. Number of rows: this statistic that depicts how many data points or observations are in the dataset.\n",
        "#It provides a sense of the dataset's size and scope, critical for understanding the overall context of the data.\n",
        "\n",
        "num_rows = len(dataset)\n",
        "\n",
        "# 2. Value lengths (minimum, maximum, median, and average): Measuring the length of values in a dataset allows to understand the distribution and variability of data.\n",
        "#It helps to identify potential outliers or anomalies and assess whether the data conforms to expected patterns.\n",
        "\n",
        "value_lengths = dataset['variable'].str.len()\n",
        "min_value_length = value_lengths.min()\n",
        "max_value_length = value_lengths.max()\n",
        "median_value_length = value_lengths.median()\n",
        "average_value_length = value_lengths.mean()\n",
        "\n",
        "# 3. Null values (number and percentage): Identifying the number or percentage of null values is essential for data quality assessment.\n",
        "#Handling missing data appropriately is vital for ensuring the accuracy and reliability of any analysis or modeling.\n",
        "\n",
        "null_values = dataset.isnull().sum()\n",
        "percentage_null_values = (null_values / num_rows) * 100\n",
        "\n",
        "# 4. Distinct values (i.e. cardinality): This statistic reveals the diversity and uniqueness of values within a column.\n",
        "#It is particularly valuable for categorical variables, as it helps to understand the number of different categories or classes present in the data.\n",
        "\n",
        "distinct_values = dataset['accident_index'].nunique()\n",
        "\n",
        "# 5. Uniqueness: it is the ratio of distinct values to the total number of rows, which provides insights into the relative diversity of the data.\n",
        "#A low uniqueness indicates that a few values dominate the dataset, which might impact analysis outcomes.\n",
        "\n",
        "uniqueness = distinct_values / num_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVPQeZ_hSTkf"
      },
      "outputs": [],
      "source": [
        "print(\"A. Cardinalities:\")\n",
        "print(\"1. Number of Rows:\", num_rows)\n",
        "print(\"2. Value Lengths - Minimum:\", min_value_length)\n",
        "print(\"   Value Lengths - Maximum:\", max_value_length)\n",
        "print(\"   Value Lengths - Median:\", median_value_length)\n",
        "print(\"   Value Lengths - Average:\", average_value_length)\n",
        "print(\"3. Null Values - Number:\", null_values)\n",
        "print(\"   Null Values - Percentage:\", percentage_null_values)\n",
        "print(\"4. Distinct Values (Cardinality):\", distinct_values)\n",
        "print(\"5. Uniqueness:\", uniqueness)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQsifky-QL4m"
      },
      "source": [
        "####B. Value Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxQFyDhkPanZ"
      },
      "outputs": [],
      "source": [
        "# 6. Histogram (equi-width):  Frequency histograms offer a visual representation of the distribution of values within a column.\n",
        "#They allow to assess data patterns, such as skewness, central tendency, and the presence of multiple modes.\n",
        "\n",
        "plt.hist(value_lengths, bins='auto', edgecolor='k')\n",
        "plt.xlabel(\"Value Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Value Lengths Histogram (Equi-Width)\")\n",
        "plt.show()\n",
        "\n",
        "# 7. Constancy (Frequency of the most frequent value): Constancy measures how frequently the most common value occurs within a column.\n",
        "#It helps identify whether a single value dominates the data, potentially highlighting anomalies or issues with data collection.\n",
        "\n",
        "most_frequent_value = dataset['accident_year'].mode().values[0]\n",
        "constancy = (dataset['accident_year'] == most_frequent_value).sum() / num_rows\n",
        "\n",
        "# 8. Quartiles: Quartiles divide numerical data into four equal parts, each containing 25% of the data points.\n",
        "#They are crucial for understanding the spread and central tendency of numeric values, making them valuable for data analysis and visualization.\n",
        "\n",
        "quartiles = np.percentile(dataset['accident_year'].dropna().astype(float), [25, 50, 75])\n",
        "\n",
        "# 9. First digit distribution (Benford's Law):  Analyzing the distribution of the first digit in numeric values can be used to detect anomalies or fraud.\n",
        "#It is a statistical test based on Benford's Law, which states the expected distribution of first digits in naturally occurring data.\n",
        "def first_digit_distribution(data_series):\n",
        "    first_digits = data_series.astype(str).str.replace(\".\", \"\").str.strip().str[0]\n",
        "    return dict(collections.Counter(first_digits))\n",
        "\n",
        "first_digit_dist = first_digit_distribution(dataset['accident_year'].dropna().astype(float))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5zze6hnXaA-"
      },
      "outputs": [],
      "source": [
        "print(\"6. Histogram (Equi-Width):\")\n",
        "# Histogram already displayed\n",
        "print(\"7. Constancy (Frequency of Most Frequent Value):\", constancy)\n",
        "print(\"8. Quartiles:\", quartiles)\n",
        "print(\"9. First Digit Distribution (Benford's Law):\")\n",
        "for digit, count in first_digit_dist.items():\n",
        "    print(f\"   Digit {digit}: {count} occurrences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOjmV3L_QTTI"
      },
      "source": [
        "####C. Patterns, Data Types, and Domains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUn2n7uRQaEP"
      },
      "outputs": [],
      "source": [
        "# 10. Data type (assuming varchar as a generic DBMS-specific type): Knowing the concrete database management system (DBMS)-specific data type (e.g., varchar, timestamp) is crucial for compatibility, data storage, and querying.\n",
        "data_type = 'varchar'\n",
        "\n",
        "# 11. Size (maximum number of digits in numeric values): It is important for assessing precision and deciding how to handle numerical values in calculations or aggregations.\n",
        "size2 = dataset['accident_year'].apply(lambda x: len(str(x)))\n",
        "size3 = size2.value_counts()\n",
        "\n",
        "# 12. Decimals (maximum number of decimals in numeric values): Knowing the maximum number of decimals helps maintain precision during calculations and conversions.\n",
        "import re\n",
        "decimal_places = dataset['accident_year'].apply(lambda x: len(re.findall(r'\\.\\d+', str(x))))\n",
        "max_decimal_places = decimal_places.max()\n",
        "\n",
        "# 13. Patterns (histogram of value patterns): It provides insights into the structure of the data, particularly for alphanumeric or complex data. It can help identify regular expressions or formats within the data.\n",
        "pattern_lengths = dataset['accident_year'].astype(str).apply(lambda x: len(set(x)))\n",
        "\n",
        "plt.hist(pattern_lengths, bins='auto', edgecolor='k')\n",
        "plt.xlabel(\"Pattern Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Pattern Lengths Histogram\")\n",
        "plt.show()\n",
        "\n",
        "# 14. Data class: Determining the semantic, generic data type (e.g., code, indicator, text, date/time) helps in understanding the purpose and meaning of the data within a column.\n",
        "data_class = 'generic'\n",
        "\n",
        "# 15. Domain (classification of semantic domain): Classifying data into semantic domains (e.g., credit card, first name, city, phenotype) provides context and informs data validation, data cleansing, and data transformation processes.\n",
        "domain = 'uncategorized'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XRKDu9_TwVd"
      },
      "outputs": [],
      "source": [
        "print(\"10. Data Type:\", data_type)\n",
        "print(\"11. Size (Maximum Number of Digits):\", size3.index[0])\n",
        "print(\"12. Decimals (Maximum Number of Decimals):\", max_decimal_places)\n",
        "print(\"13. Patterns (Pattern Lengths Histogram):\") # Histogram already displayed\n",
        "print(\"14. Data Class:\", data_class)\n",
        "print(\"15. Domain:\", domain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qL0yGoQ8_wR"
      },
      "source": [
        "##Entity resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtqN_Eo0-Sp2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install recordlinkage\n",
        "import recordlinkage as rl\n",
        "\n",
        "!pip install py_stringmatching\n",
        "from py_stringmatching import Levenshtein\n",
        "from py_stringmatching import Jaro\n",
        "from py_stringmatching import Affine\n",
        "from sklearn.preprocessing import MinMaxScaler as mms\n",
        "lev = Levenshtein()\n",
        "jaro = Jaro()\n",
        "aff = Affine()\n",
        "\n",
        "acm_df = pd.read_csv(\"ACM.csv\")\n",
        "dblp_df = pd.read_csv(\"DBLP2.csv\", encoding=\"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr2KkF88AWVB"
      },
      "source": [
        "Compare every single record in the dataset (ACM.csv) with all the records in (DBLP2.csv) and find the similar records (records that represent the same publication). ACM contains the following attributes: id,\"title\",\"authors\",\"venue\",\"year\". ACM contains the following attributes: id,\"title\",\"authors\",\"venue\",\"year\". To compare two records, follow the steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al6b4xxvAkwm"
      },
      "source": [
        "A) Ignore the pub_id.\n",
        "\n",
        "B) Change all alphabetical characters into lowercase.\n",
        "\n",
        "C) Convert multiple spaces to one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxY7ByN5AyDu"
      },
      "outputs": [],
      "source": [
        "#a:\n",
        "#pub_id is ignored automatically in the similarity functions, because only the specific ['column'] is used in the dataframe\n",
        "\n",
        "#b:\n",
        "def to_lower(df):\n",
        "  for i in range(len(df)):\n",
        "    for j in range(len(df.columns)):\n",
        "      lower = str(df.iloc[i,j]).lower()\n",
        "      df.iloc[i,j] = lower\n",
        "  return df\n",
        "\n",
        "acm_df = to_lower(acm_df)\n",
        "dblp_df = to_lower(dblp_df)\n",
        "\n",
        "#c:\n",
        "import re\n",
        "def replace_spaces(df):\n",
        "  for i in range(len(df)):\n",
        "    for j in range(len(df.columns)):\n",
        "      sentence = str(df.iloc[i][j])\n",
        "      result = re.sub(' +', ' ', sentence)\n",
        "      df.iloc[i][j] = result\n",
        "  return df\n",
        "\n",
        "acm_df = replace_spaces(acm_df)\n",
        "dblp_df = replace_spaces(dblp_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGppNcLkhQ6a"
      },
      "source": [
        "D) Use Levenshtein similarity (for comparing the values in the title attribute and compute the score (st). (MED refers to the minimum edit distance and |Si| is the number of characters in string Si)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsbeSvQI8-Iq"
      },
      "outputs": [],
      "source": [
        "#d:\n",
        "def levenshtein_sim():\n",
        "  lev_scores = []\n",
        "\n",
        "  #Iterate over the datasets, compute the minimal edit distance using the py_stringmatching library and manually convert to the Levenshtein similarity\n",
        "  for i in range(len(acm_df)):\n",
        "      for j in range(len(dblp_df)):\n",
        "          min_edit_dist = lev.get_raw_score(acm_df['title'][i], dblp_df['title'][j])\n",
        "          max_length = max(len(acm_df['title'][i]), len(dblp_df['title'][j]))\n",
        "          comp_lev = 1 - min_edit_dist/max_length\n",
        "          lev_scores.append((acm_df['title'][i], dblp_df['title'][j], comp_lev))\n",
        "\n",
        "  #St is a list with all the computed Levenshtein scores and for clarity lev_df is a dataset where you can see the compared titles and their similarity score\n",
        "  lev_df = pd.DataFrame(lev_scores, columns=['ACM Title', 'DBLP Title', 'Levenshtein Score'])\n",
        "  St = lev_df['Levenshtein Score']\n",
        "  return lev_df, St\n",
        "\n",
        "lev_df, St = levenshtein_sim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBQmXjsomV46"
      },
      "outputs": [],
      "source": [
        "#e:\n",
        "def jaro_sim():\n",
        "  jaro_scores = []\n",
        "\n",
        "  #Iterate over the datasets and compute the Jaro score using the py_stringmatching library\n",
        "  for i in range(len(acm_df)):\n",
        "      for j in range(len(dblp_df)):\n",
        "          comp_jaro = jaro.get_raw_score(acm_df['authors'][i], dblp_df['authors'][j])\n",
        "          jaro_scores.append((acm_df['authors'][i], dblp_df['authors'][j], comp_jaro))\n",
        "\n",
        "  #Sa is a list with all the computed Jaro scores and for clarity jaro_df is a dataset where you can see the compared authors and their similarity score\n",
        "  jaro_df = pd.DataFrame(jaro_scores, columns=['ACM Authors', 'DBLP Authors', 'Jaro Score'])\n",
        "  Sa = jaro_df['Jaro Score']\n",
        "  return jaro_df, Sa\n",
        "\n",
        "jaro_df, Sa = jaro_sim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHWdVR7cAhVn"
      },
      "source": [
        "F) Use a modified version of the affine similarity that is scaled to the interval [0, 1] for the venue attribute (Sc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMJn1KAX_exa"
      },
      "outputs": [],
      "source": [
        "#f:\n",
        "def aff_sim():\n",
        "\n",
        "  aff_scores = []\n",
        "\n",
        "  #Iterate over the datasets and compute the Affine gap score using the py_stringmatching library\n",
        "  for i in range(len(acm_df)):\n",
        "    for j in range(len(dblp_df)):\n",
        "        score = aff.get_raw_score(acm_df.iloc[i]['venue'], dblp_df.iloc[j]['venue'])\n",
        "        aff_scores.append(score)\n",
        "\n",
        "  #Scale the scores to [0, 1] range using MinMaxScaler()\n",
        "  scaler = mms()\n",
        "  sc_scaled = scaler.fit_transform(np.array(aff_scores).reshape(-1, 1)).flatten() #The reshape makes sure it is a 2D array and flatten takes away the brackets []\n",
        "  aff_scores_scaled = []\n",
        "\n",
        "  #Now add the scaled scores to the records\n",
        "  index = 0\n",
        "  for i in range(len(acm_df)):\n",
        "    for j in range(len(dblp_df)):\n",
        "      aff_scores_scaled.append((acm_df.iloc[i]['venue'], dblp_df.iloc[j]['venue'], sc_scaled[index]))\n",
        "      index += 1\n",
        "\n",
        "  #Sc is a list with all the scaled Affine scores and for clarity aff_df is a dataset where you can see the compared venues and their similarity score\n",
        "  aff_df = pd.DataFrame(aff_scores_scaled, columns=['ACM Venue', 'DBLP Venue', 'Scaled Affine Score'])\n",
        "  Sc = aff_df['Scaled Affine Score']\n",
        "  return aff_df, Sc\n",
        "\n",
        "aff_df, Sc = aff_sim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGIk5EikA4eE"
      },
      "source": [
        "G) Use Match (1) / Mismatch (0) for the year (Sy).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qARvjbGaTnuT"
      },
      "outputs": [],
      "source": [
        "#g:\n",
        "def compute_matches():\n",
        "  match_year = []\n",
        "\n",
        "  #Iterate over the datasets and compute the match/mismatch score (either 1 or 0)\n",
        "  for i in range(len(acm_df)):\n",
        "    for j in range(len(dblp_df)):\n",
        "          if acm_df['year'][i] == dblp_df['year'][j]:\n",
        "            match_year.append((acm_df['year'][i], dblp_df['year'][j], 1))\n",
        "          else:\n",
        "            match_year.append((acm_df['year'][i], dblp_df['year'][j], 0))\n",
        "\n",
        "  #Sy is a list with all the match/mismatch scores and for clarity match_df is a dataset where you can see the compared years and their similarity score\n",
        "  match_df = pd.DataFrame(match_year, columns=['ACM year', 'DBLP year', 'Match (1) or Mismatch (0)'])\n",
        "  Sy = match_df['Match (1) or Mismatch (0)']\n",
        "  return match_df, Sy\n",
        "\n",
        "match_df, Sy = compute_matches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLh-ueIwTYTy"
      },
      "source": [
        "H) Use the formula rec_sim = w1 * st + w2 * sa + w3 * sc + w4 *sy to combine the scores and compute the final score, where the sum of the 4 attributes = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iklxdCXToHK"
      },
      "outputs": [],
      "source": [
        "#h:\n",
        "def rec_sim():\n",
        "  w1 = 0.25\n",
        "  w2 = 0.25\n",
        "  w3 = 0.25\n",
        "  w4 = 0.25\n",
        "  rec_sim = w1 * St + w2 * Sa + w3 * Sc + w4 * Sy\n",
        "  return rec_sim\n",
        "\n",
        "rec_sim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcOycVNHTbHR"
      },
      "source": [
        "I) Report the records with rec_sim > 0.7 as duplicate records by storing the ids of both records in a list."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def duplicate_records():\n",
        "  rec_scores = rec_sim()\n",
        "  duplicate_records = []\n",
        "\n",
        "  #Iterate over the datasets and note down both id's and their combined similarity score\n",
        "  for i in range(len(acm_df)):\n",
        "    for j in range(len(dblp_df)):\n",
        "      duplicate_records.append((acm_df.iloc[i]['id'], dblp_df.iloc[j]['id'], rec_scores[i * len(dblp_df) + j]))\n",
        "\n",
        "  #Create a new dataframe containing both id's and their combined similarity scores, then filter to show only similarity scores larger than 0.7\n",
        "  combined_df = pd.DataFrame(duplicate_records, columns=['ACM_ID', 'DBLP_ID', 'Rec_Sim_Score'])\n",
        "  filtered_df = combined_df[combined_df['Rec_Sim_Score'] > 0.7]\n",
        "  return filtered_df\n",
        "\n",
        "duplicate_records()\n"
      ],
      "metadata": {
        "id": "uDHXbhJw_hHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCfZWW3TenN"
      },
      "source": [
        "J) In the table DBLP-ACM_perfectMapping.csv, you can find the actual mappings (the ids of the correct duplicate records). Compute the precision of this method by counting the number of duplicate records that you discovered correctly. That is, among all the reported similar records by your method, how many pairs exist in the file DBLP-ACM_perfectMapping.csv."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_precision():\n",
        "  #Read in the perfect mapping csv and make sure to also convert to the desired format using our definitions from b and c above\n",
        "  dblp_perf = pd.read_csv('DBLP-ACM_perfectMapping.csv')\n",
        "  dblp_perf = to_lower(dblp_perf)\n",
        "  dblp_perf = replace_spaces(dblp_perf)\n",
        "\n",
        "  duplicate_df = duplicate_records()\n",
        "  total_duplicates = len(duplicate_df)\n",
        "\n",
        "  correct_duplicates = 0\n",
        "  for i in range(total_duplicates):\n",
        "    if (\n",
        "            (duplicate_df.iloc[i]['ACM_ID'] in dblp_perf['idACM'].values) and\n",
        "            (duplicate_df.iloc[i]['DBLP_ID'] in dblp_perf['idDBLP'].values)\n",
        "        ):\n",
        "            correct_duplicates += 1\n",
        "\n",
        "  precision = correct_duplicates/total_duplicates\n",
        "  print(\"The precision of the record similarity measure is:\", str(precision))\n",
        "\n",
        "compute_precision()"
      ],
      "metadata": {
        "id": "f6eVEQTCGP8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLJehGrhTiA-"
      },
      "source": [
        "K) Record the running time of the method. You can observe that the program takes a long time to get the results. What can you do to reduce the running time? (Just provide clear discussion – no need for implementing the ideas.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtjtoXcdTkeK"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "import random\n",
        "starttime = timeit.default_timer()\n",
        "levenshtein_sim()\n",
        "time1 = timeit.default_timer() - starttime\n",
        "\n",
        "starttime2 = timeit.default_timer()\n",
        "jaro_sim()\n",
        "time2 = timeit.default_timer() - starttime2\n",
        "\n",
        "starttime3 = timeit.default_timer()\n",
        "aff_sim()\n",
        "time3 = timeit.default_timer() - starttime3\n",
        "\n",
        "starttime4 = timeit.default_timer()\n",
        "compute_matches()\n",
        "time4 = timeit.default_timer() - starttime4\n",
        "\n",
        "starttime5 = timeit.default_timer()\n",
        "rec_sim()\n",
        "time5 = timeit.default_timer() - starttime5\n",
        "\n",
        "starttime6 = timeit.default_timer()\n",
        "duplicate_records()\n",
        "time6 = timeit.default_timer() - starttime6\n",
        "\n",
        "starttime7 = timeit.default_timer()\n",
        "compute_precision()\n",
        "time7 = timeit.default_timer() - starttime7\n",
        "\n",
        "total_runtime = time1+time2+time3+time4+time5+time6+time7\n",
        "print(\"Total runtime:\", str(total_runtime))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGTkOLs-ik89"
      },
      "source": [
        "To reduce the running time, we could consider the following strategies:\n",
        "\n",
        "1. **Early Exit Optimization**: It can be added an early exit condition to break out of the calculation if the edit distance exceeds a certain threshold or if it becomes larger than a predefined maximum value.\n",
        "\n",
        "2. **Memoization**: Implement memoization (caching) to store the results of previously computed Levenshtein distances for pairs of strings.\n",
        "\n",
        "3. **Parallel Processing**: If there is a large number of string comparisons to make, you can parallelize the calculations using multi-threading or multiprocessing.  \n",
        "\n",
        "4. **Indexing and Filtering**: When comparing a large dataset against a smaller set of potential matches, indexing and filtering the potential matches first based on some criteria may reduce the number of expensive distance calculations.\n",
        "\n",
        "5. **Library Optimizations**: Specialized libraries or functions that are optimized for string similarity calculations.\n",
        "\n",
        "6. **Limit String Length**: limiting the length of the strings being compared, by truncating or preprocessing long strings to make them more manageable for similarity calculations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKQ_nyryzNUB"
      },
      "source": [
        "## Enhanced Entity Resolution Method using Shingling, MinHash, and Locality Sensitive Hashing (LSH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwqSIhsdzR9D"
      },
      "source": [
        "Concatenating the values in each record into one single string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-DrT3SnzU6v"
      },
      "outputs": [],
      "source": [
        "def concat_records(df):\n",
        "  records = []\n",
        "  for i in range(len(df)):\n",
        "    row = df.iloc[i, :]\n",
        "    row = row.str.cat(sep=' ')\n",
        "    records.append(row)\n",
        "  return records\n",
        "\n",
        "acm_records = concat_records(acm_df)\n",
        "dblp_records = concat_records(dblp_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing all alphabetical characters into lowercase.\n"
      ],
      "metadata": {
        "id": "mByy3ryZZf2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lower(records):\n",
        "  for i in range(len(records)):\n",
        "    lower = str(records[i]).lower()\n",
        "    records[i] = lower\n",
        "  return records\n",
        "\n",
        "acm_records = to_lower(acm_records)\n",
        "dblp_records = to_lower(dblp_records)"
      ],
      "metadata": {
        "id": "GiUu5eoZZiIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert multiple spaces to one.\n"
      ],
      "metadata": {
        "id": "tU56WOz5Zk9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_spaces(records):\n",
        "  for i in range(len(records)):\n",
        "    record = records[i]\n",
        "    result = re.sub(' +', ' ', record)\n",
        "    records[i] = result\n",
        "  return records\n",
        "\n",
        "acm_records = replace_spaces(acm_records)\n",
        "dblp_records = replace_spaces(dblp_records)"
      ],
      "metadata": {
        "id": "PvGzpIUIZn_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine the records from both tables into one big list as we did during the lab."
      ],
      "metadata": {
        "id": "5nnu7IYXZqeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_records = acm_records + dblp_records\n",
        "print(combined_records)"
      ],
      "metadata": {
        "id": "ei6qsGMrZucm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the functions in the tutorials from lab 5 to compute the shingles, the minhash signature and the similarity."
      ],
      "metadata": {
        "id": "zQ0GKImJZzKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shingle(text: str, k: int)->set:\n",
        "    \"\"\"\n",
        "    Create a set of 'shingles' from the input text using k-shingling.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to be converted into shingles.\n",
        "        k (int): The length of the shingles (substring size).\n",
        "\n",
        "    Returns:\n",
        "        set: A set containing the shingles extracted from the input text.\n",
        "    \"\"\"\n",
        "    shingle_set = []\n",
        "    for i in range(len(text) - k+1):\n",
        "        shingle_set.append(text[i:i+k])\n",
        "    return set(shingle_set)\n",
        "\n",
        "def build_vocab(shingle_sets: list)->dict:\n",
        "    \"\"\"\n",
        "    Constructs a vocabulary dictionary from a list of shingle sets.\n",
        "\n",
        "    This function takes a list of shingle sets and creates a unified vocabulary\n",
        "    dictionary. Each unique shingle across all sets is assigned a unique integer\n",
        "    identifier.\n",
        "\n",
        "    Parameters:\n",
        "    - shingle_sets (list of set): A list containing sets of shingles.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A vocabulary dictionary where keys are the unique shingles and values\n",
        "      are their corresponding unique integer identifiers.\n",
        "\n",
        "    Example:\n",
        "    sets = [{\"apple\", \"banana\"}, {\"banana\", \"cherry\"}]\n",
        "    build_vocab(sets)\n",
        "    {'apple': 0, 'cherry': 1, 'banana': 2}  # The exact order might vary due to set behavior\n",
        "    \"\"\"\n",
        "    full_set = {item for set_ in shingle_sets for item in set_}\n",
        "    vocab = {}\n",
        "    for i, shingle in enumerate(list(full_set)):\n",
        "        vocab[shingle] = i\n",
        "    return vocab\n",
        "\n",
        "def one_hot(shingles: set, vocab: dict):\n",
        "    vec = np.zeros(len(vocab))\n",
        "    for shingle in shingles:\n",
        "        idx = vocab[shingle]\n",
        "        vec[idx] = 1\n",
        "    return vec\n",
        "\n",
        "\n",
        "def get_shingles_2(): #code block turned into function to record run time\n",
        "  k = 3\n",
        "  sentences = combined_records\n",
        "  shingles = []\n",
        "  for sentence in sentences:\n",
        "      shingles.append(shingle(sentence,k))\n",
        "  vocab = build_vocab(shingles)\n",
        "  shingles_1hot = []\n",
        "  for shingle_set in shingles:\n",
        "      shingles_1hot.append(one_hot(shingle_set,vocab))\n",
        "  shingles_1hot = np.stack(shingles_1hot)\n",
        "  return vocab, shingles_1hot, shingles\n",
        "\n",
        "vocab = get_shingles_2()[0]\n",
        "shingles_1hot = get_shingles_2()[1]\n",
        "shingles = get_shingles_2()[2]\n",
        "\n",
        "def get_minhash_arr(num_hashes:int,vocab:dict):\n",
        "    \"\"\"\n",
        "    Generates a MinHash array for the given vocabulary.\n",
        "\n",
        "    This function creates an array where each row represents a hash function and\n",
        "    each column corresponds to a word in the vocabulary. The values are permutations\n",
        "    of integers representing the hashed value of each word for that particular hash function.\n",
        "\n",
        "    Parameters:\n",
        "    - num_hashes (int): The number of hash functions (rows) to generate for the MinHash array.\n",
        "    - vocab (dict): The vocabulary where keys are words and values can be any data\n",
        "      (only keys are used in this function).\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: The generated MinHash array with `num_hashes` rows and columns equal\n",
        "      to the size of the vocabulary. Each cell contains the hashed value of the corresponding\n",
        "      word for the respective hash function.\n",
        "\n",
        "    Example:\n",
        "    vocab = {'apple': 1, 'banana': 2}\n",
        "    get_minhash_arr(2, vocab)\n",
        "    # Possible output:\n",
        "    # array([[1, 2],\n",
        "    #        [2, 1]])\n",
        "    \"\"\"\n",
        "    length = len(vocab.keys())\n",
        "    arr = np.zeros((num_hashes,length))\n",
        "    for i in range(num_hashes):\n",
        "        permutation = np.random.permutation(len(vocab.keys())) + 1\n",
        "        arr[i,:] = permutation.copy()\n",
        "    return arr.astype(int)\n",
        "\n",
        "def get_signature(minhash:np.ndarray, vector:np.ndarray):\n",
        "    \"\"\"\n",
        "    Computes the signature of a given vector using the provided MinHash matrix.\n",
        "\n",
        "    The function finds the nonzero indices of the vector, extracts the corresponding\n",
        "    columns from the MinHash matrix, and computes the signature as the minimum value\n",
        "    across those columns for each row of the MinHash matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - minhash (np.ndarray): The MinHash matrix where each column represents a shingle\n",
        "      and each row represents a hash function.\n",
        "    - vector (np.ndarray): A vector representing the presence (non-zero values) or\n",
        "      absence (zero values) of shingles.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: The signature vector derived from the MinHash matrix for the provided vector.\n",
        "\n",
        "    Example:\n",
        "    minhash = np.array([[2, 3, 4], [5, 6, 7], [8, 9, 10]])\n",
        "    vector = np.array([0, 1, 0])\n",
        "    get_signature(minhash, vector)\n",
        "    output:array([3, 6, 9])\n",
        "    \"\"\"\n",
        "    idx = np.nonzero(vector)[0].tolist()\n",
        "    shingles = minhash[:,idx]\n",
        "    signature = np.min(shingles,axis=1)\n",
        "    return signature\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection_size = len(set1.intersection(set2))\n",
        "    union_size = len(set1.union(set2))\n",
        "    return intersection_size / union_size if union_size != 0 else 0.0\n",
        "\n",
        "def compute_signature_similarity(signature_1, signature_2):\n",
        "    \"\"\"\n",
        "    Calculate the similarity between two signature matrices using MinHash.\n",
        "\n",
        "    Parameters:\n",
        "    - signature_1: First signature matrix as a numpy array.\n",
        "    - signature_matrix2: Second signature matrix as a numpy array.\n",
        "\n",
        "    Returns:\n",
        "    - Estimated Jaccard similarity.\n",
        "    \"\"\"\n",
        "    # Ensure the matrices have the same shape\n",
        "    if signature_1.shape != signature_2.shape:\n",
        "        raise ValueError(\"Both signature matrices must have the same shape.\")\n",
        "    # Count the number of rows where the two matrices agree\n",
        "    agreement_count = np.sum(signature_1 == signature_2)\n",
        "    # Calculate the similarity\n",
        "    similarity = agreement_count / signature_2.shape[0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def get_sigs(): #turned into function to compute run time later\n",
        "  minhash_arr =  get_minhash_arr(100,vocab)\n",
        "  signatures = []\n",
        "  for vector in shingles_1hot:\n",
        "    signatures.append(get_signature(minhash_arr,vector))\n",
        "  signatures = np.stack(signatures)\n",
        "  signatures.shape\n",
        "  return signatures\n",
        "\n",
        "signatures = get_sigs()\n",
        "\n",
        "\n",
        "print(compute_signature_similarity(signatures[2],signatures[3]))\n",
        "print(jaccard_similarity(shingles[3],shingles[2]))"
      ],
      "metadata": {
        "id": "EW1rGH69Z2VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the top 2224 candidates from the LSH algorithm, compare them to the actual mappings in the file DBLP-ACM_perfectMapping.csv and compute the precision of the method"
      ],
      "metadata": {
        "id": "gFFl4fSFZ9V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "from random import shuffle\n",
        "from itertools import combinations\n",
        "class LSH:\n",
        "    \"\"\"\n",
        "    Implements the Locality Sensitive Hashing (LSH) technique for approximate\n",
        "    nearest neighbor search.\n",
        "    \"\"\"\n",
        "    buckets = []\n",
        "    counter = 0\n",
        "\n",
        "    def __init__(self, b: int):\n",
        "        \"\"\"\n",
        "        Initializes the LSH instance with a specified number of bands.\n",
        "\n",
        "        Parameters:\n",
        "        - b (int): The number of bands to divide the signature into.\n",
        "        \"\"\"\n",
        "        self.b = b\n",
        "        for i in range(b):\n",
        "            self.buckets.append({})\n",
        "\n",
        "    def make_subvecs(self, signature: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Divides a given signature into subvectors based on the number of bands.\n",
        "\n",
        "        Parameters:\n",
        "        - signature (np.ndarray): The MinHash signature to be divided.\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray: A stacked array where each row is a subvector of the signature.\n",
        "        \"\"\"\n",
        "        l = len(signature)\n",
        "        assert l % self.b == 0\n",
        "        r = int(l / self.b)\n",
        "        subvecs = []\n",
        "        for i in range(0, l, r):\n",
        "            subvecs.append(signature[i:i+r])\n",
        "        return np.stack(subvecs)\n",
        "\n",
        "    def add_hash(self, signature: np.ndarray):\n",
        "        \"\"\"\n",
        "        Adds a signature to the appropriate LSH buckets based on its subvectors.\n",
        "\n",
        "        Parameters:\n",
        "        - signature (np.ndarray): The MinHash signature to be hashed and added.\n",
        "        \"\"\"\n",
        "        subvecs = self.make_subvecs(signature).astype(str)\n",
        "        for i, subvec in enumerate(subvecs):\n",
        "            subvec = ','.join(subvec)\n",
        "            if subvec not in self.buckets[i].keys():\n",
        "                self.buckets[i][subvec] = []\n",
        "            self.buckets[i][subvec].append(self.counter)\n",
        "        self.counter += 1\n",
        "\n",
        "    def check_candidates(self) -> set:\n",
        "        \"\"\"\n",
        "        Identifies candidate pairs from the LSH buckets that could be potential near duplicates.\n",
        "\n",
        "        Returns:\n",
        "        - set: A set of tuple pairs representing the indices of candidate signatures.\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "        for bucket_band in self.buckets:\n",
        "            keys = bucket_band.keys()\n",
        "            for bucket in keys:\n",
        "                hits = bucket_band[bucket]\n",
        "                if len(hits) > 1:\n",
        "                    candidates.extend(combinations(hits, 2))\n",
        "        return set(candidates)\n",
        "\n",
        "\n",
        "def apply_lsh():\n",
        "  b = 10   # number of buckets\n",
        "  lsh = LSH(b)\n",
        "  for signature in signatures:\n",
        "      lsh.add_hash(signature)\n",
        "  candidate_pairs = lsh.check_candidates()\n",
        "  len(candidate_pairs)\n",
        "  return candidate_pairs\n",
        "\n",
        "candidate_pairs = apply_lsh()\n",
        "top_candidates = list(candidate_pairs)[:2224]"
      ],
      "metadata": {
        "id": "xvVSBrNeZ-Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#actually comparing top candidates with correct mappings:\n",
        "# we might have to check precision formula because precision formula is num true pos/ (num true pos + false pos)\n",
        "dblp_key = pd.read_csv('DBLP-ACM_perfectMapping.csv')\n",
        "\n",
        "def compute_precision(top_candidates):\n",
        "  false_count = 0\n",
        "  correct_count = 0\n",
        "  dblp_keys_id = dblp_key['idACM']\n",
        "  for i in range(len(top_candidates)):\n",
        "    if top_candidates[i][0] in dblp_keys_id or top_candidates[i][1] in dblp_keys_id:\n",
        "      correct_count+=1\n",
        "    else:\n",
        "      false_count+=1\n",
        "  precision = correct_count/(correct_count+false_count)\n",
        "  print(\"Precision:\", str(precision))\n",
        "compute_precision(top_candidates)\n"
      ],
      "metadata": {
        "id": "FDZ-TXwJaF0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the running time of the method"
      ],
      "metadata": {
        "id": "06883sQbaA81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "import random\n",
        "starttime = timeit.default_timer()\n",
        "get_shingles_2()\n",
        "time1 = timeit.default_timer() - starttime\n",
        "\n",
        "starttime2 = timeit.default_timer()\n",
        "get_sigs()\n",
        "time2 = timeit.default_timer() - starttime2\n",
        "\n",
        "starttime3 = timeit.default_timer()\n",
        "apply_lsh()\n",
        "time3 = timeit.default_timer() - starttime3\n",
        "\n",
        "starttime4 = timeit.default_timer()\n",
        "compute_precision(top_candidates)\n",
        "time4 = timeit.default_timer() - starttime4\n",
        "\n",
        "total_runtime = time1+time2+time3+time4\n",
        "print(\"Total runtime:\", str(total_runtime))"
      ],
      "metadata": {
        "id": "oxDgz5q6218l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The precision for the method in Part 2 was much higher than the precision in Part 1 and the runtime in Part 2 was also much faster than Part 1. Overall, this would make the method in Part 2 a better choice for entity resolution."
      ],
      "metadata": {
        "id": "VWCjjbZPaLd3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OoFGMGASz4J"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyf65t9_SeWi"
      },
      "source": [
        "1. Computing the correlation between the different columns after removing the outcome column.\n",
        "\n",
        "2. Removing the disguised values from the table. We need to remove the values that equal to 0 from columns BloodPressure, SkinThickness and BMI as these are missing values but they have been replaced by the value 0. Remove the value but keep the record (i.e.) change the value to null.\n",
        "\n",
        "3. Filling the cells with null using the mean values of the records that have the same class label.\n",
        "\n",
        "4. Computing the correlation between the different columns.\n",
        "\n",
        "5. Comparing the values from this step with the values in the first step (just mention the most important changes (if any)) and comment on your findings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eaatS85gKGt"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('diabetes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nm7HAcVgNSl"
      },
      "outputs": [],
      "source": [
        "#1)\n",
        "df2 = df.drop(['Outcome'], axis=1)\n",
        "print(df2.corr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kat4XGSzgZDG"
      },
      "outputs": [],
      "source": [
        "#2\n",
        "import numpy as np\n",
        "cols = ['BloodPressure', 'SkinThickness', 'BMI']\n",
        "for col in df.columns:\n",
        "  df[col] = df[col].replace(0, np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP1Fuxvk0Cz-"
      },
      "outputs": [],
      "source": [
        "#3\n",
        "for i in range(len(df)):\n",
        " for j in range(len(df.columns)):\n",
        "   if pd.isna(df.iloc[i][j]):\n",
        "     if df.loc[i]['Outcome']==1.0:\n",
        "       cls = df[df['Outcome']==1.0]\n",
        "     else:\n",
        "       cls = df[df['Outcome']!=1.0]\n",
        "     col_name = df.columns[j]\n",
        "     col_avg = cls.iloc[:, j].mean()\n",
        "     df.iloc[i, j] = col_avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28xYHTQJk2Vl"
      },
      "outputs": [],
      "source": [
        "#4\n",
        "df3 = df.drop(['Outcome'], axis=1)\n",
        "correlation_matrix = df3.corr()\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLL7AQuOxG2y"
      },
      "source": [
        "5: The main difference between the correlations in part 4 and part 1 is that the correlations in part 1 were calculated with rows that contained values of 0, whereas these rows were ignored in part 4 where they had null values. The correlations not involving the blood pressure, skin thickness or BMI columns are the same in both parts, but in general, while the correlations are relatively similar for those columns, the correlations in part 4 are slightly higher because the null rows have been omitted.\n",
        "\n",
        "\n",
        "*   Step 1 focuses on understanding **independent variable relationships and potential multicollinearity**.\n",
        "*   In contrast, Step 4 provides a **broader view of correlations and aiding variable selection for predictive modeling**.\n",
        "*   The main difference is that Step 4 includes **correlations between predictors and the outcome**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}